from datetime import datetime, timedelta
import psycopg2
import os

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator

# ============================================================================
#   DAG D'INITIALISATION - EX√âCUTION UNIQUE
# ============================================================================
# 
# üéØ CE DAG G√àRE L'INITIALISATION COMPL√àTE:
# 
# 1Ô∏è‚É£ Full historical weather (2024-01-01 ‚Üí NOW)
# 2Ô∏è‚É£ Full historical energy consumption (2024-01-01 ‚Üí NOW)
# 3Ô∏è‚É£ Marquer la DB comme initialis√©e
# 
# ‚ö†Ô∏è  Ce DAG ne doit s'ex√©cuter qu'UNE SEULE FOIS
# ============================================================================

def get_db_connection():
    """Cr√©e une connexion √† la base de donn√©es PostgreSQL"""
    # En environnement Docker, utiliser le nom du service, pas localhost
    host = os.getenv("GAUSSDB_HOST")
    if not host or host == "localhost":
        # Essayer le nom du service PostgreSQL dans docker-compose
        # Adapter selon votre configuration
        host = os.getenv("POSTGRES_HOST", "postgres")
    
    return psycopg2.connect(
        host=host,
        port=os.getenv("GAUSSDB_PORT", "5432"),
        database=os.getenv("GAUSSDB_DB_SILVER", "silver"),
        user=os.getenv("GAUSSDB_USER", "postgres"),
        password=os.getenv("GAUSSDB_PASSWORD", "postgres"),
        sslmode=os.getenv("GAUSSDB_SSLMODE", "disable")
    )

def mark_initialization_complete(**context):
    """
    Marque la base de donn√©es comme initialis√©e en cr√©ant une table de flag.
    """
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        print("=" * 80)
        print("üîó Connexion √† la base de donn√©es √©tablie")
        print("=" * 80)
        
        # Cr√©er une table de m√©tadonn√©es si elle n'existe pas
        cur.execute("""
            CREATE TABLE IF NOT EXISTS pipeline_metadata (
                key VARCHAR(100) PRIMARY KEY,
                value TEXT,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Marquer comme initialis√©
        cur.execute("""
            INSERT INTO pipeline_metadata (key, value, updated_at)
            VALUES ('initialization_complete', 'true', CURRENT_TIMESTAMP)
            ON CONFLICT (key) DO UPDATE 
            SET value = 'true', updated_at = CURRENT_TIMESTAMP
        """)
        
        conn.commit()
        
        print("=" * 80)
        print("‚úÖ BASE DE DONN√âES MARQU√âE COMME INITIALIS√âE")
        print("=" * 80)
        
    except Exception as e:
        print(f"‚ùå Erreur lors du marquage: {e}")
        if conn:
            conn.rollback()
        raise
    finally:
        if conn:
            cur.close()
            conn.close()

default_args = {
    "owner": "hamza",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="initialization_pipeline",
    description="Initialisation unique: Weather + Energy historique complet",
    default_args=default_args,
    start_date=datetime(2025, 11, 11),
    schedule_interval=None,  # Manual trigger only
    catchup=False,
    max_active_runs=1,
    tags=["renewstation", "initialization", "one-time"],
) as dag:

    # ========================================================================
    #   √âTAPE 1: M√âT√âO HISTORIQUE COMPL√àTE
    # ========================================================================
    
    start_init = EmptyOperator(task_id="start_initialization")
    
    full_weather_backfill = BashOperator(
        task_id="full_weather_historical",
        bash_command=(
            "cd /opt/airflow && "
            "PYTHONPATH=/opt/airflow "
            "python -m src.pipeline.generator.weather_forecasting --mode full"
        ),
        execution_timeout=timedelta(minutes=30),
    )
    
    # ========================================================================
    #   √âTAPE 2: √âNERGIE HISTORIQUE COMPL√àTE
    # ========================================================================
    
    full_energy_backfill = BashOperator(
        task_id="full_energy_historical",
        bash_command=(
            "cd /opt/airflow && "
            "PYTHONPATH=/opt/airflow "
            "python -m src.pipeline.generator.energy_cons_generator --mode full"
        ),
        execution_timeout=timedelta(minutes=45),
    )
    
    # ========================================================================
    #   √âTAPE 3: MARQUER COMME INITIALIS√â
    # ========================================================================
    
    mark_complete = PythonOperator(
        task_id="mark_initialization_complete",
        python_callable=mark_initialization_complete,
        provide_context=True,
    )
    
    init_complete = EmptyOperator(task_id="initialization_complete")
    
    # ========================================================================
    #   D√âPENDANCES
    # ========================================================================
    
    start_init >> full_weather_backfill >> full_energy_backfill >> mark_complete >> init_complete


# ============================================================================
#   NOTES D'UTILISATION
# ============================================================================
# 
# üìã Pour ex√©cuter ce DAG:
#    1. Aller dans l'interface Airflow
#    2. Trouver "initialization_pipeline"
#    3. Cliquer sur "Trigger DAG"
#    4. Attendre la fin de l'ex√©cution (peut prendre 30-45 minutes)
# 
# ‚ö†Ô∏è  Ce DAG ne doit √™tre ex√©cut√© qu'UNE SEULE FOIS!
# 
# ‚úÖ Apr√®s ex√©cution, le DAG "update_and_prediction_pipeline" pourra fonctionner
#
# üîß Configuration requise:
#    Variables d'environnement √† d√©finir:
#    - GAUSSDB_HOST ou POSTGRES_HOST (nom du service PostgreSQL)
#    - GAUSSDB_PORT (par d√©faut: 5432)
#    - GAUSSDB_DB_SILVER (par d√©faut: silver)
#    - GAUSSDB_USER (par d√©faut: postgres)
#    - GAUSSDB_PASSWORD (par d√©faut: postgres)
# 
# ============================================================================